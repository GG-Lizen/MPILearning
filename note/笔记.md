# 笔记

## Eratosthenes 筛法

### 串行算法

![基本数论类算法- 辗转相除和素数筛| 春水煎茶- 王超的个人博客](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQa3oAEZxJ9cff5Ar9TJmAxDkZ6uRRZynxpnw&usqp=CAU)

1. 创建一个自然数 2，3，4,⋯，n的列表，其中所有的自然数都没有被标记
2. 令k=2，它是列表中第一个未被标记的数
2.  重复下面步骤直到k^2>n为止:
（a）把k^2和n之间的是k倍数的数都标记出來
（b） 找出比k大的未被标记的数中最小的那个，令k等于这个数
4. 列表中未被标记的数就是素数

### 并行性来源

核心是标记数组，进行域分解，每个域执行的任务相同。

### 数据分解

负载均衡，采用分布式方案。

![image-20220409025308126](https://s2.loli.net/2022/04/09/HAwlcZCgn627ohG.png)



### 实现

- 输入：上界，输出素数数，运行时间
- 运行条件：Master控制的最后一个元素的平方大于上界
- 思想：串行改并行。

```
#define BLOCK_LOW(id, p, n) ((id) * (n) / (p))
#define BLOCK_HIGH(id, p, n) (BLOCK_LOW((id) + 1, p, n) - 1)
#define BLOCK_SIZE(id, p, n) \
    (BLOCK_LOW((id) + 1, p, n) - BLOCK_LOW(id, p, n))  // p96 wrong?
#define BLOCK_OWNER(index, p, n) (((p) * ((index) + 1) - 1) - 1) / n
```



```
#include <math.h>
#include <stdio.h>
#include "MyMPI.h"
#include "mpi.h"
#define MIN(a, b) ((a) < (b) ? (a) : (b))

int main(int argc, char* argv[]) {
    int count;           /* Local prime count */
    double elapsed_time; /* Parallel execution time */
    int first;           /* Index of first multiple */
    int global_count;    /* Global prime count */
    int high_value;      /* Highest value on this proc */
    int i;
    int id;         /* Process ID number */
    int index;      /* Index of current prime */
    int low_value;  /* Lowest value on this proc */
    char* marked;   /* Portion of 2,...,'n' */
    int n;          /* Sieving from 2, ..., 'n' */
    int p;          /* Number of processes */
    int proc0_size; /* Size of proc 0's subarray */
    int prime;      /* Current prime */
    int size;       /* Elements in 'marked' */

    MPI_Init(&argc, &argv);

    /* Start the timer */
    MPI_Barrier(MPI_COMM_WORLD);
    elapsed_time = -MPI_Wtime();
    MPI_Comm_rank(MPI_COMM_WORLD, &id);
    MPI_Comm_size(MPI_COMM_WORLD, &p);

    // must specified upper bound
    // !id => master
    if (argc != 2) {
        if (!id)
            printf("Command line: %s <m>\n", argv[0]);
        MPI_Finalize();
        exit(1);
    }

    n = atoi(argv[1]);

    /* Figure out this process's share of the array, as
       well as the integers represented by the first and
       last array elements */

    low_value = 2 + BLOCK_LOW(id, p, n - 1);  //进程的第一个数 2开始，n是上界
    high_value = 2 + BLOCK_HIGH(id, p, n - 1);  //进程的最后一个数
    size = BLOCK_SIZE(id, p, n - 1);            //进程处理的数组大小

    /* Bail out if all the primes used for sieving are
       not all held by process 0 */

    proc0_size = (n - 1) / p;

    if ((2 + proc0_size) <
        (int)sqrt((double)n)) {  // ? 2开始  2 + proc0_size 0所拥有的最大数
        if (!id)
            printf("Too many processes\n");
        MPI_Finalize();
        exit(1);
    }

    /* Allocate this process's share of the array. */

    marked = (char*)malloc(size);

    if (marked == NULL) {
        printf("Cannot allocate enough memory\n");
        MPI_Finalize();
        exit(1);
    }

    for (i = 0; i < size; i++)
        marked[i] = 0;
    if (!id)
        index = 0;
    prime = 2;  //从素数2开始
    do {
        //确定素数的第一个倍数的下标（局部下标）
        if (prime * prime > low_value)
            first = prime * prime - low_value;
        else {
            if (!(low_value % prime))
                first = 0;
            else
                first = prime -
                        (low_value % prime);  // (p)--(r)--(lv)--(p-r)--(p+p)
        }

        for (i = first; i < size; i += prime)
            marked[i] = 1;
        if (!id) {
            while (marked[++index])
                ;
            prime = index + 2;  //  下标从0开始 但数从2开始
        }
        MPI_Bcast(&prime, 1, MPI_INT, 0, MPI_COMM_WORLD);
    } while (prime * prime <= n);

    count = 0;
    for (i = 0; i < size; i++)
        if (!marked[i])
            count++;
    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    /* Stop the timer */

    elapsed_time += MPI_Wtime();

    /* Print the results */

    if (!id) {
        printf("There are %d primes less than or equal to %d\n", global_count,
               n);
        printf("Total elapsed time: %10.6f\n", elapsed_time);
    }
    MPI_Finalize();
    return 0;
}
```

### 改进

- 删除偶数
  - 消除广播
    - 重组循环（cache命中率）

### 总结

- 找串行可并行写并行
- 负载均衡
- 通信

## Floyd

求图中点到点的最短距离

### 串行算法

```
for k<-0 to n-1
	for i<-0 to n-1
		for j<-0 to n-1
			a[i,j]<-min(a[i,j],a[i,k]+a[k,j]);
```

### 并行算法

#### 划分

域分解

#### 通信

- 存在的问题：进行第k次迭代的时候，能同时更新
  - 可以，第k次迭代第k行并未改变

所以可以广播第k行到每一个任务

#### 聚集

行列广播时间都相同，都是要广播第k行的n个元素

考虑到C中数组按行存储，所以按行分解

#### 映射

同埃氏筛法

### 实现

1. 读矩阵
   - p-1进程读m,n，广播
   - 各进程申请自己的内存
   - p-1进程读入和分发矩阵数据
2. 计算最短路径
   - 接受第k行矩阵数据
   - 更新计算

```
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include "MyMPI.h"
typedef int dtype;
#define MPI_TYPE MPI_INT
int main(int argc, char* argv[]) {
    dtype** a;      /* Doubly-subscripted array*/
    dtype* storage; /* Local portion of array elements */
    int i, j, k;
    int id; /* Process rank */
    int m;  /* Rows in matrix */
    int n;  /* Columns in matrix */
    int p;  /* Number of processes */
    void compute_shortest_paths(int, int, int**, int);
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &id);
    MPI_Comm_size(MPI_COMM_WORLD, &p);

    read_row_striped_matrix(argv[1], (void*)&a, (void*)&storage, MPI_TYPE, &m,
                            &n, MPI_COMM_WORLD);

    if (m != n)
        terminate(id, "Matrix must be squareln");
    print_row_striped_matrix((void**)a, MPI_TYPE, m, n, MPI_COMM_WORLD);
    compute_shortest_paths(id, p, (dtype**)a, n);
    print_row_striped_matrix((void**)a, MPI_TYPE, m, n, MPI_COMM_WORLD);
    MPI_Finalize();
}

void compute_shortest_paths(int id, int p, dtype** a, int n) {
    int i, j, k;
    int offset; /* Local index of broadcast row */
    int root;   /* Process controlling row to be beast */
    int* tmp;   /* Holds the broadcast row */
    tmp = (dtype*)malloc(n * sizeof(dtype));
    for (k = 0; k < n; k++) {
        root = BLOCK_OWNER(k, p, n);
        if (root == id) {
            offset = k - BLOCK_LOW(id, p, n);
            for (j = 0; j < n; j++)
                tmp[j] = a[offset][j];
        }
        MPI_Bcast(tmp, n, MPI_TYPE, root, MPI_COMM_WORLD);
        for (i = 0; i < BLOCK_SIZE(id, p, n); i++)
            for (j = 0; j < n; j++)
                a[i][j] = MIN(a[i][j], a[i][k] + tmp[j]);
    }
    free(tmp);
}
```

fread读的是二进制



## 性能分析

### 加速比

并行计算操作：

- 必须串行执行的计算
- 可以并行执行的计算
- 并行开销（通信和冗余计算）



加速比=串行程序执行时间/并行程序执行时间
$$
\Psi(n,p) 表示在p个处理器上解决规模为n的问题时的加速比
\\\sigma(n)  表示计算中内在的串行部分
\\\varphi(n) 表示可以并行执行的计算
\\\kappa(n,p) 表示并行计算开销所需的时间
$$


表达公式：
$$
\large \Psi(n,p)\leq\tfrac{\sigma(n) +\varphi(n)}{\sigma(n) +\varphi(n)/p+\kappa(n,p)}
$$

### 效率

效率=串行执行时间/（使用的处理器数量 * 并行执行的时间）

公式：
$$
\large \varepsilon \leq \tfrac{\sigma(n) +\varphi(n)}{p\sigma(n) +\varphi(n)+p\kappa(n,p)}
$$

### Amdahl定律

求取并行解决问题的加速比上界。 
$$
\large \Psi(n,p)\leq\tfrac{\sigma(n) +\varphi(n)}{\sigma(n) +\varphi(n)/p+\kappa(n,p)}\leq\tfrac{\sigma(n) +\varphi(n)}{\sigma(n) +\varphi(n)/p}\tag{1}
$$
用f表示计算中串行执行所占比例
$$
\large f=\tfrac{\sigma(n) }{\sigma(n) +\varphi(n)}
$$
带入（1）
$$
\large \Psi(n,p)\leq\tfrac{1}{f+(1-f)/p}
$$
amdahl效应：计算规模增大，加速比同时也增大

### Gustafson-Barsis定律

s:并行计算过程中串行部分时间比例
$$
\large s=\tfrac{\sigma(n) +\varphi(n)}{\sigma(n) +\varphi(n)/p}
$$

$$
\large \Psi(n,p)\leq s+(1-s)p
$$

$$
\large \Psi(n,p)\leq p+(1-p)s
$$
不同于Amdahl从串行程序，Gustafson-Barsis从并行程序出发

提供了比例加速比的估算

### Karp-Flatt量度

程序在p个处理器上的执行时间：
$$
T(n,p)=\sigma(n) +\varphi(n)/p+\kappa(n,p)
$$


定义：
$$
e=(\sigma(n)+\kappa(n,p))/T(n,1)
$$
则：
$$
T(n,p)=T(n,1)e +T(n,1)(1-e)/p
$$
定义：
$$
\psi(n,p)=T(n,1)/T(n,p)
$$
得：
$$
\large e=\tfrac{1/\psi-1/p}{1-1/p}\tag{Karp-Flatt量度}
$$


若计算的e没有随着处理器的个数增加而增加，可以看出性能不好的主要原因是有限的并行性，及大部分计算是串行的；若随着处理器的个数增加而稳定增加，可以看出加速比差的原因并行开销（进程启动，通信，同步，或系统结构上的限制）

用于预测程序在更多处理器上的性能

### 等效指标

## O[(n/p)log(n/p)]+ 𝑝^2  log⁡〖𝑝+"(n/p)logp" 〗]Merge Sort

### 串行算法

```
int min(int x, int y) {
    return x < y ? x : y;
}
void merge_sort(int arr[], int len) {
    int *a = arr;
    int *b = (int *) malloc(len * sizeof(int));
    int seg, start;
    for (seg = 1; seg < len; seg += seg) {
        for (start = 0; start < len; start += seg * 2) {
            int low = start, mid = min(start + seg, len), high = min(start + seg * 2, len);
            int k = low;
            int start1 = low, end1 = mid;
            int start2 = mid, end2 = high;
            while (start1 < end1 && start2 < end2)
                b[k++] = a[start1] < a[start2] ? a[start1++] : a[start2++];
            while (start1 < end1)
                b[k++] = a[start1++];
            while (start2 < end2)
                b[k++] = a[start2++];
        }
        int *temp = a;
        a = b;
        b = temp;
    }
    if (a != arr) {
        int i;
        for (i = 0; i < len; i++)
            b[i] = a[i];
        b = a;
    }
    free(b);
}
```



### 规则取样并行排序PSRS

1. 阶段一：
   
   - 划分为p段，分别在p个程序上进行排序
   
   - 采用快速排序（或归并排序）进行排列
	
	- 按照如下规则采样
   
     - 
   
     $$
     \large\lfloor\tfrac{i * datalength}{p} \rfloor，i=0,1,2……(p-1)
     $$
   
     $$
     datalength=n/p
     $$
   
   - 让某一进程从其余p-1个进程各接受p个样本
   
2. 阶段二：

   - 收集数据的进程对p^2个元素进行排序
   - 收集数据的进程广播p-1个中点到其他进程

3. 阶段三：

   - 各进程依据中值将自己的列表分成p部分，所有进程进行一次全交换
   
4. 阶段四：

   - 所有进程合并其p个片段成一个列表


![在这里插入图片描述](https://img-blog.csdnimg.cn/20200602033408111.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21pbmF0b3Nhbg==,size_16,color_FFFFFF,t_70#pic_center)

## 矩阵向量乘法

### 串行算法

```
输入：a[0..m-1,0..n-1]//m x n matrix
	 b[0..m-1]//n 维向量
输出：c[0..m-1]//m 维向量
for i <- 0 to m - 1
	c[i] <- 0 
	for j <- 0 to n - 1
    	c[i] <- c[i] + a[i,j] x b[j]
    endfor
end for
```

### 划分

域分解：

- 按行分解
- 按列分解
- 棋盘式分解

### 按行分解

若m=n 矩阵，矩阵向量乘法时间复杂度O(n^2),则并行部分时间复杂度为O(n^2/p)，通信复杂度O(logp + n)，总复杂度O(n^2+logp + n)

n>>p => 通信复杂度O(n)

等效率函数：n>=Cp 

内存使用函数：M(n)=n^2

可扩展函数：M(Cp)/p=C^2p

#### MPI_Allgatherv函数

[MPI_Allgatherv](https://www.rookiehpc.com/mpi/docs/mpi_allgatherv.php) is a variant of [MPI_Allgather](https://www.rookiehpc.com/mpi/docs/mpi_allgather.php); it collects data from all processes in a given communicator and stores the data collected in the receive buffer of each process.

[MPI_Allgatherv](https://www.rookiehpc.com/mpi/docs/mpi_allgatherv.php) allows the messages received to have different lengths and be stored at arbitrary locations in the receive buffer

```
int MPI_Allgatherv(const void* buffer_send,
                   int count_send,
                   MPI_Datatype datatype_send,
                   void* buffer_recv,
                   const int* counts_recv,
                   const int* displacements,
                   MPI_Datatype datatype_recv,
                   MPI_Comm communicator);
buffer_send 此进程要发送的數据的起始地址：
count_send 此进程要发送的数据的个数；
counts_recv 包含要从每个进程（包括自身）接收的数据个数的数组，
displacements 从每个进程接收的数据项在缓冲区中的偏移量：
communicator本操作所在通信域。
```

```

 * +-----------+ +-----------+ +-------------------+ 
 * | Process 0 | | Process 1 | |     Process 2     |
 * +-+-------+-+ +-+-------+-+ +-+-------+-------+-+
 *   | Value |     | Value |     | Value | Value |
 *   |  100  |     |  101  |     |  102  |  103  |
 *   +-------+     +-------+     +-------+-------+
 *      |                |            |     |
 *      |                |            |     |
 *      |                |            |     |
 *      |                |            |     |
 *      |                |            |     |
 *      |                |            |     |
 *   +-----+-----+-----+-----+-----+-----+-----+
 *   | 100 |  0  |  0  | 101 |  0  | 102 | 103 |
 *   +-----+-----+-----+-----+-----+-----+-----+
 *   |               Each process              |
 *   +-----------------------------------------+
 *
```



![](/home/gg/Downloads/send buffer.jpg)

### 按列分解

n>=Cp 

#### 按列读取

1. 一个进程读取矩阵的一行至一个临时的缓冲区
2. 这个进程分发缓冲区的元素至所有进程。
3. 重复以上操作。

- MPI_Scatterv

```


```

- MPI_Gatherv

  ```
  int MPI_Gatherv(const void* buffer_send,
                  int count_send,
                  MPI_Datatype datatype_send,
                  void* buffer_recv,
                  const int* counts_recv,
                  const int* displacements,
                  MPI_Datatype datatype_recv,
                  int root,
                  MPI_Comm communicator);
  
  ```

- MPI_Alltoallv

```
int MPI_Alltoallv(const void* buffer_send,
                  const int* counts_send,
                  const int* displacements_send,
                  MPI_Datatype datatype_send,
                  void* buffer_recv,
                  const int* counts_recv,
                  const int* displacements_recv,
                  MPI_Datatype datatype_recv,
                  MPI_Comm communicator);

```

#### 并行算法

1. 每个进程分配到矩阵A 的第i列和向量b的第i个元素，将每个元素与b[i]相乘得到a[0,i]b[i],a[1,i]b[i]....a[n-1,i]b[i]
2. 把自己不需要n-1个结果分发到其他进程，同时向其他进程收集需要的n-1个结果（全交换）
3. 进程i将它的所有元素求和得到c[i]



### 棋盘式分解

# 静态库

http://c.biancheng.net/view/7168.html

```
gg@ubuntu:~/cpp/parallel computing/ByLizen$ mpicc -c Myutils.c
gg@ubuntu:~/cpp/parallel computing/ByLizen$ ar rcs libMyutils.a Myutils.o
gg@ubuntu:~/cpp/parallel computing/ByLizen$ mpicc -o test test.c -L. -l Myutils
```

```
myid is 0 receive count is 12 bytes
myid is 0 receive element size is 9
myid is 0  recvvector index i  is  0 element is 0
myid is 0  recvvector index i  is  1 element is 1
myid is 0  recvvector index i  is  2 element is 2
myid is 0  recvvector index i  is  3 element is 0
myid is 0  recvvector index i  is  4 element is 1
myid is 0  recvvector index i  is  5 element is 2
myid is 0  recvvector index i  is  6 element is 0
myid is 0  recvvector index i  is  7 element is 1
myid is 0  recvvector index i  is  8 element is 2
myid is 1 receive count is 0 bytes
myid is 1 receive element size is 0
myid is 2 receive count is 12 bytes
myid is 2 receive element size is 9
myid is 2  recvvector index i  is  0 element is 3
myid is 2  recvvector index i  is  1 element is 4
myid is 2  recvvector index i  is  2 element is 5
myid is 2  recvvector index i  is  3 element is 3
myid is 2  recvvector index i  is  4 element is 4
myid is 2  recvvector index i  is  5 element is 5
myid is 2  recvvector index i  is  6 element is 3
myid is 2  recvvector index i  is  7 element is 4
myid is 2  recvvector index i  is  8 element is 5
```

